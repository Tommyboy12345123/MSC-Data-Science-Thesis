{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "reNuadu0Z3vs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from gensim import corpora\n",
        "from gensim.models import TfidfModel, LdaMulticore\n",
        "from wordcloud import WordCloud\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJqBT3WqFqQa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FxhwCaoZqjL"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DATA/IDX_FILE_TEXT_DAT.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vWomCruYlLQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U96Fr4t7Z8WB"
      },
      "outputs": [],
      "source": [
        "def clean(x):\n",
        "  for i in tqdm(range(len(df))):\n",
        "    try:\n",
        "      x = re.sub(r\"[^a-zA-Z0-9 ]\", \" \", df['text'][i])\n",
        "      x = x.lower()\n",
        "      x = re.sub(' {2,}', ' ', x)\n",
        "      df.loc[i,'text'] = x\n",
        "    except:\n",
        "      continue\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2uiTqX2b_3H"
      },
      "outputs": [],
      "source": [
        "clean(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xh7vNBw-l3G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaNG4Lf0J6qZ"
      },
      "outputs": [],
      "source": [
        "my_file = open(\"/content/drive/MyDrive/StopWords_Generic.txt\", \"r\")\n",
        "content = my_file.read()\n",
        "content_list_1 = content.split(\"\\n\")\n",
        "my_file.close()\n",
        "content_list_1 = [x.lower() for x in content_list_1]\n",
        "\n",
        "my_file = open(\"/content/drive/MyDrive/StopWords_GenericLong.txt\", \"r\")\n",
        "content = my_file.read()\n",
        "content_list_2 = content.split(\"\\n\")\n",
        "my_file.close()\n",
        "content_list_2 = [x.lower() for x in content_list_2]\n",
        "\n",
        "my_file = open(\"/content/drive/MyDrive/StopWords_DatesandNumbers.txt\", \"r\")\n",
        "content = my_file.read()\n",
        "content_list_3 = content.split(\"\\n\")\n",
        "my_file.close()\n",
        "content_list_3 = [x.lower() for x in content_list_3]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoU6ozIrKNJe"
      },
      "outputs": [],
      "source": [
        "content_list_1.extend(content_list_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCmC245WK6h9"
      },
      "outputs": [],
      "source": [
        "content_list_1.extend(content_list_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vERH35w_LN0o"
      },
      "outputs": [],
      "source": [
        "extra_stopwords = content_list_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--FQq3czLUM6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpEcKIrlc7QR"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def lemmetize(words):\n",
        "    lemmatized_words = [WordNetLemmatizer().lemmatize(w.lower()) for w in words]\n",
        "    return lemmatized_words\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "fin_stop_words = (\"million\",\"including\",\"billion\",\"december\",\"january\")\n",
        "stop_words.update(extra_stopwords)\n",
        "\n",
        "# removing stop words, numbers , removing punctuations and special characters and spaces\n",
        "def remove_stopwords(words):\n",
        "    filtered = [re.sub(r'[^\\w\\s]','',w) for w in words if not re.sub(r'[^\\w\\s]','',w) in stop_words and  not re.sub(r'[^\\w\\s]','',w).isnumeric() and not re.search('^\\s*[0-9]',re.sub(r'[^\\w\\s]','',w)) and len(re.sub(r'[^\\w\\s]','',w)) > 3  ]\n",
        "    return filtered\n",
        "\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "   \n",
        "#ps = PorterStemmer() \n",
        "ps = SnowballStemmer(\"english\") \n",
        "\n",
        "def stem_words(words):\n",
        "    stemmed = [ps.stem(w) for w in words]\n",
        "    return stemmed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vrsudjFQj0_"
      },
      "outputs": [],
      "source": [
        "df['lists'] = [list() for x in range(len(df.index))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkxxYsix6bhh"
      },
      "outputs": [],
      "source": [
        "df_1 = df[:12500]\n",
        "df_2 = df[12500:25000]\n",
        "df_3 = df[25000:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del df\n",
        "df = None"
      ],
      "metadata": {
        "id": "VSx-wcGRw231"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQwViP0b6n5i"
      },
      "outputs": [],
      "source": [
        "df_1 = df_1.reset_index(drop=True)\n",
        "df_2 = df_2.reset_index(drop=True)\n",
        "df_3 = df_3.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "for i in tqdm(range(len(df_1))):\n",
        "  try:\n",
        "    lemmatized_words = [WordNetLemmatizer().lemmatize(w.lower()) for w in nltk.word_tokenize(df_1['text'][i])]\n",
        "    x = remove_stopwords(lemmatized_words)\n",
        "\n",
        "    df_1.loc[i,'lists'].append(x)\n",
        "    df_1.loc[i, 'text'] = np.nan\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "df_1.to_csv('/content/drive/MyDrive/Colab Notebooks/DATA/DATASET 1 SIMILARITY.csv')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "df_1.to_csv('df_1.csv')\n",
        "files.download('df_1.csv')"
      ],
      "metadata": {
        "id": "zkdYyn6B3eZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQX-47HOfwy0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "for i in tqdm(range(len(df_2))):\n",
        "  try:\n",
        "    lemmatized_words = [WordNetLemmatizer().lemmatize(w.lower()) for w in nltk.word_tokenize(df_2['text'][i])]\n",
        "    x = remove_stopwords(lemmatized_words)\n",
        "\n",
        "    df_2.loc[i,'lists'].append(x)\n",
        "    df_2.loc[i, 'text'] = np.nan\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "df_2.to_csv('/content/drive/MyDrive/Colab Notebooks/DATA/DATASET 2 SIMILARITY.csv')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "df_2.to_csv('df_2.csv')\n",
        "files.download('df_2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "for i in tqdm(range(len(df_3))):\n",
        "  try:\n",
        "    lemmatized_words = [WordNetLemmatizer().lemmatize(w.lower()) for w in nltk.word_tokenize(df_3['text'][i])]\n",
        "    x = remove_stopwords(lemmatized_words)\n",
        "\n",
        "    df_3.loc[i,'lists'].append(x)\n",
        "    df_3.loc[i, 'text'] = np.nan\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "df_3.to_csv('/content/drive/MyDrive/Colab Notebooks/DATA/DATASET 2 SIMILARITY.csv')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "df_3.to_csv('df_3.csv')\n",
        "files.download('df_3.csv')"
      ],
      "metadata": {
        "id": "CdfIQgyo06nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DATA/DOC SIMILARITY/DATASET 1 SIMILARITY.csv')\n",
        "df_2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DATA/DOC SIMILARITY/DATASET 2 SIMILARITY.csv')\n",
        "df_3 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/DATA/DOC SIMILARITY/DATASET 3 SIMILARITY.csv')"
      ],
      "metadata": {
        "id": "QHATS_byoQYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df_1,df_2],ignore_index = True)\n",
        "\n",
        "df = pd.concat([df,df_3],ignore_index = True)"
      ],
      "metadata": {
        "id": "cSP4cIbco3Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USo088OeFK8i"
      },
      "outputs": [],
      "source": [
        "df.sort_values(by=['permno','date'])\n",
        "\n",
        "df = df.reset_index(drop = True)             \n",
        "\n",
        "df['counter'] = df.groupby(['permno']).cumcount()+1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M2VAiTuApsxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBhuqEcMXKNi"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "for i in tqdm(range(len(df))):\n",
        "  l1 =[];l2 =[]\n",
        "  try:\n",
        "    if df['counter'][i] == 1:\n",
        "      df.loc[i,'cosine'] = 0\n",
        "      df.loc[i,'jaccard'] = 0\n",
        "      df.loc[i, 'text'] = np.nan\n",
        "    else:\n",
        "        set_1 = set(df['lists'][i-1].split(','))\n",
        "        set_2 = set(df['lists'][i].split(','))\n",
        "\n",
        "        rvector = set_1.union(set_2) \n",
        "\n",
        "        counter_1 = Counter(df['lists'][i-1].split(','))\n",
        "        counter_2 = Counter(df['lists'][i].split(','))\n",
        "\n",
        "\n",
        "\n",
        "        for w in rvector:\n",
        "          if w in set_1: l1.append(counter_1[w]) \n",
        "          else: l1.append(0)\n",
        "          if w in set_2: l2.append(counter_2[w])\n",
        "          else: l2.append(0)\n",
        "\n",
        "        c = 0\n",
        "\n",
        "        for g in range(len(rvector)):\n",
        "          c += l1[g]*l2[g]\n",
        "\n",
        "        cosine = c / float(((sum([pow(item, 2) for item in l1])**0.5) * (sum([pow(item, 2) for item in l2])**0.5)))\n",
        "\n",
        "        df.loc[i,'cosine'] = cosine\n",
        "\n",
        "        intersection = set_1.intersection(set_2)\n",
        "\n",
        "        jaccard = float(len(intersection)) / len(rvector)\n",
        "\n",
        "        df.loc[i,'jaccard'] = jaccard\n",
        "\n",
        "        df.loc[i, 'text'] = np.nan\n",
        "\n",
        "  except:\n",
        "    continue"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sBIr3rGov6no"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['lists','text','counter','Unnamed: 0','Unnamed: 0.1','Unnamed: 0.1.1'], 1)"
      ],
      "metadata": {
        "id": "qboMga1NqfNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/DATA/SIMILARITY RESULTS FINAL.csv')"
      ],
      "metadata": {
        "id": "TdoZ0Zalp5jo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "DOCUMENT SIMILARITY.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}